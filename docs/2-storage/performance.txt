We can use the 'time' program on linux to work out the speed of doing commands
Using this to benchmark different filesystem layouts.

Filled folder with 50,000 tiny numbered txt files.

Accessing 1.txt takes the same amount of time as 31307.txt. 
This shows when we know the file we are looking for, we access it very quickly, regardless of sequential or random access.
1:
real	0m0.002s
31307:
real	0m0.002s

Seeking to the middle of a 100mb file does not seem to have any significant performance issue.

Random Access Took 0.000083s
Sequential Access Took 0.000088s
Random Access Took 0.000150s
Sequential Access Took 0.000110s
Random Access Took 0.000109s
Sequential Access Took 0.000098s
Random Access Took 0.000090s
Sequential Access Took 0.000078s

Tests on a sample of 70 chunks of data show that accessing the first file is incredibly quick, but accessing random files take the same amount of time within a margin of error.
This test has the same results as our test of 50,000 small files. This justifies the simple one folder layout of the system.

0.bin
real	0m0.023s
user	0m0.004s
sys	0m0.016s
1.bin
real	0m3.711s
user	0m0.008s
sys	0m0.100s
2.bin
real	0m3.671s
user	0m0.000s
sys	0m0.088s
22.bin
real	0m3.687s
user	0m0.008s
sys	0m0.080s
15.bin
real	0m4.885s
user	0m0.000s
sys	0m0.096s
55.bin
real	0m3.669s
user	0m0.000s
sys	0m0.084s
2.bin
real	0m3.649s
user	0m0.000s
sys	0m0.088s
